"""
This file contains a list of functions that are used
to preprocess the graph files before they can be used by Trackgae
1 - Load graph of each timestep
2 - Cluster the timesteps if no groundtruth clusters are provided
3 - load attributes
4 - Calculate cluster attributes
5 - Calculate Netsimile features for the clusters
6 - Generate membership matrix
7 - Generate burt matrix
"""

# Libraries
import os
import time

import networkx as nx
import numpy as np
from scipy import stats as st
from scipy.sparse import csr_matrix

import datapreparation.cluster as ct
import datapreparation.netsimile_features as nf
import datapreparation.utils as ut
import datapreparation.utils_dancer as du

# Variables

# location of the data
data_directory = ""

# graph has attributes
with_attributes = False

# ground truth clusters are provided with the graph
with_clusters = False

# ground truth sequences are provided with the graph
with_ground_truth_sequences = False

# list of all the graphs
timesteps_graphs = []
# list of the clusters
clusters = []
# A lookup that contains the snapshot Id of the cluster Plus its size
clusters_lookup = []
# the attributes of every graph
attributes = []
# the clusters attributes form by aggregating the attributes of its nodes
clusters_attributes = []
# the Netsimile features of the cluster
clusters_topo_features = []

# the membership matrix that indicates which clusters each node belongs to
membership_matrix = []
# the burt matrix that indicates the number of intersecting nodes between the clusters
burt_matrix = []
# the burt matrix normalized
burt_norm = []

# the ground truth sequences (if provided with the dataset)
ground_truth = []

# the number of snapshots
number_timesteps = 0

# the total number of clusters
number_clusters = 0

# the clusters embeddings generated by the pretraining step of the TrackGCN
all_clusters_embeddings = []


def preprocess_data(data_directory, file_extension, with_attributes, with_clusters, with_ground_truth_sequences,
                    nodes_from_edges,
                    top_lines_to_remove=0,
                    split_char='\t',
                    print_load_details=False):
    """
    This function takes a list of graphs files (a graph for every timestep)
    and performs the pre-processing steps necessary for trackgae to properly extract the sequences
     1- clustering: it performs clustering on each timestep if no ground truth clusters are provided
     2- Membership matrix: it builds the membership matrix
     3- Burt matrix: it uses the membership matrix to build the Burt matrix which is used as the adjacency matrix of the
     super graph

    *the preprocessed data is saved in folder processed for future use

    :param data_directory: the input data path (list of graph files)
    :param file_extension: the extension of the graph files
    :param with_attributes: if the graph is attributed
    :param with_clusters: if the clusters are already provided or not
    (if not Markov clustering will be applied on each timestep to extract the clusters)
    :param with_ground_truth_sequences: if the ground truth sequences are provided with the dataset
    :param nodes_from_edges: if the nodes are generated from the edge file or are in a separate file
    :param top_lines_to_remove: number of lines to ignore from the top of each graph file
    :param split_char: separation character in the graph file
    :param print_load_details: print the details of the graphs that are loaded from the files
    :return: timesteps_graphs : list of time steps  where each time step is a graph
             clusters: list of all time steps where each time step is a list of clusters
             clusters_lookup: clusters lookup with the format: timestep ID | cluster index in this timestep | cluster size
             membership_matrix: a binary matrix where the lines are the nodes and the columns are the clusters
             (1: node belongs to cluster)
             burt_matrix: a square matrix where the rows and columns are the clusters that contains
             the number of nodes that are shared by a pair of clusters
             attributes: list of attributes related to each graph (if the graph has attributes)
    """

    # files where to write the numpy arrays that were processed
    timesteps_graphs_file_path = data_directory + "\\processed\\timesteps_graphs.nx"
    clusters_file_path = data_directory + "\\processed\\clusters"
    clusters_lookup_file_path = data_directory + "\\processed\\clusters_lookup"
    membership_matrix_file_path = data_directory + "\\processed\\membership_matrix"
    burt_matrix_file_path = data_directory + "\\processed\\burt_matrix"
    burt_matrix_csv_file_path = data_directory + "\\processed\\burt_matrix_csv.csv"
    attributes_file_path = data_directory + "\\processed\\attributes"

    if nodes_from_edges:
        nodes_directory_path = ""
    else:
        nodes_directory_path = data_directory + "\\nodes"

    # extract the graph info of each file and load it into memory as a networkx graph
    timesteps_graphs = ut.load_graph_directory(data_directory + "\\edges", nodes_directory_path, file_extension,
                                               top_lines_to_remove, split_char,
                                               print_load_details,
                                               timesteps_graphs_file_path)


    if with_attributes:
        # if the graph is attributed, load the attributes of each timestep graph
        attributes = ut.load_attributes_directory(data_directory + "\\attributes", file_extension, 0,
                                                  attributes_file_path)
    else:
        # if not, the attributes data is left empty
        attributes = []

    if with_clusters:
        # if ground truth clusters are provided, load the clusters from the clusters folder
        clusters = ut.load_clusters_directory(data_directory + "\\clusters", file_extension, 0,
                                              clusters_file_path)
    else:
        # if not, apply Markov clustering to each time step
        clusters = ct.cluster_multiple_timesteps(timesteps_graphs, attributes, 1.4, True, clusters_file_path)

    if with_attributes:
        # build clusters attributes
        clusters_attributes = build_clusters_attributes(data_directory, clusters, attributes, timesteps_graphs)

    else:
        # if not, the cluster attributes data is left empty
        clusters_attributes = []

    clusters_topo_features = nf.build_clusters_netsimile_features(data_directory, clusters, timesteps_graphs)

    # build a cluster lookup from all the clusters of all timesteps
    # this object helps locate a cluster in the time steps for later retrieval
    clusters_lookup = ct.build_clusters_lookup(clusters, clusters_lookup_file_path)

    # build membership matrix for all clusters of all timesteps
    membership_matrix = build_membership_matrix(clusters, membership_matrix_file_path)

    # build a burt matrix from the membership matrix
    burt_matrix = build_burt_matrix(membership_matrix, burt_matrix_file_path, burt_matrix_csv_file_path)

    load_preprocessed_data(data_directory, with_attributes, with_clusters, with_ground_truth_sequences)

    return timesteps_graphs, clusters, clusters_lookup, membership_matrix, burt_matrix, attributes, clusters_attributes, clusters_topo_features


def preprocess_data_AS(data_directory, print_load_details=False):
    # preprocess the data of the dataset: AS

    if not create_processed_directory(data_directory, False, False,False):
        return

    return preprocess_data(data_directory, ".txt", False, False, True, 4, '\t', print_load_details)


def preprocess_data_yelp(data_directory, print_load_details=False):
    # preprocess the data of the dataset: Yelp

    if not create_processed_directory(data_directory, True, True, False):
        return

    return preprocess_data(data_directory, ".txt", True, True, False, False, 0, ',', print_load_details)

def preprocess_data_test(data_directory, print_load_details=False):
    # preprocess the data of the dataset: Yelp

    if not create_processed_directory(data_directory,True, True, False):
        return

    return preprocess_data(data_directory, ".txt", True, True, False, False, 0, ',', print_load_details)


def preprocess_data_DANCER(data_directory, extract_data=False, print_load_details=False):
    """
    preprocess the data generated by Dancer synthetic graph generator

    :param data_directory: the path the Dancer files
    :param extract_data: extract the data first before preprocessing them
    :param print_load_details: print the details of each graph
    :return: the results of preprocess_data
    """

    if not create_processed_directory(data_directory, True, True, False):
        return

    ground_truth_file_path = data_directory + "\\processed\\ground_truth"

    if extract_data:
        # extract the graph data before processing it
        du.extract_data_directory(data_directory, ground_truth_file_path, "graph")

    return preprocess_data(data_directory, ".graph", True, True, False, False, 0, ',', print_load_details)


def load_preprocessed_file(data_directory, file_name):
    """
    this function loads a preprocessed file into memory

    :param file_name: the file to be loaded
    (possible values: timesteps_graphs, clusters, clusters_lookup, membership_matrix, burt_matrix, ground_truth)
    :return: numpy file
    """

    preprocessed_data = ut.load_numpy_file(data_directory + "\\processed\\" + file_name + ".npy")

    return preprocessed_data


def create_processed_directory(data_directory, with_attributes, with_clusters, with_ground_truth_sequences):
    """Create base directories for the processed dataset"""
    if not os.path.isdir(data_directory + "\\processed"):
        os.mkdir(data_directory + "\\processed")
        return True
    else:
        print("Data already processed!")
        print("Please delete the directory " + data_directory + "\\processed" + " if you want to re-process the data ")
        load_preprocessed_data(data_directory, with_attributes, with_clusters, with_ground_truth_sequences)
        return False


def build_membership_matrix(clusters, membership_matrix_file_path=""):
    """
    This function builds the membership matrix:
     A binary matrix where the lines are the nodes and the columns are the clusters (1: node blongs to cluster)
    :param clusters: list of all  timesteps where each timestep is a list of clusters of that timestep
    :param membership_matrix_file_path: where to save the membership matrix numpy file
    :return: membership_matrix numpy array
    """
    start = time.time()

    print("-----------------------------------")
    print("building membership matrix...")

    membership_matrix = []
    nodes = []

    for timestep in clusters:
        for cluster in timestep:
            nodes.extend(cluster)

    nodes = sorted(list(set(nodes)))

    for node in nodes:
        node_row = []
        for timestep in clusters:
            for cluster in timestep:
                if node in cluster:
                    node_row.extend([1])
                else:
                    node_row.extend([0])
        membership_matrix.append(node_row)

    membership_matrix = np.asarray(membership_matrix, int)

    # save numpy file
    if membership_matrix_file_path:
        np.save(membership_matrix_file_path, membership_matrix)

    end = time.time()
    print("completed: ( size = " + str(membership_matrix.shape) + " in " + ut.get_elapsed(start, end) + " )")
    print("-----------------------------------")

    return membership_matrix


def build_burt_matrix(membership_matrix, burt_matrix_file_path="", burt_matrix_csv_file_path=""):
    """
    this function builds the burt matrix out of the memebership matrix
    A square matrix where the rows and columns are the clusters that contains the number of nodes that are shared by a pair of clusters
    Burt_Matrix = Transpose(Membership_Matrix) * Membership_Matrix

    :param membership_matrix: membership matrix
    :param burt_matrix_file_path: where to save the burt numpy file
    :param burt_matrix_csv_file_path : where to save the burt csv file (readable by humans)
    :return: Burt matrix numpy array
    """
    start = time.time()

    print("-----------------------------------")
    print("building burt matrix...")

    csr_transpose_membership_matrix = csr_matrix(np.transpose(membership_matrix))
    burt_matrix = csr_transpose_membership_matrix.dot(membership_matrix)

    # save numpy file
    if burt_matrix_file_path:
        np.save(burt_matrix_file_path, burt_matrix)

    # save csv file
    if burt_matrix_csv_file_path:
        with open(burt_matrix_csv_file_path, "wb") as f:
            np.savetxt(f, burt_matrix.astype(int), fmt='%i', delimiter=",")

    end = time.time()
    print("completed: ( size = " + str(burt_matrix.shape) + " in " + ut.get_elapsed(start, end) + " )")
    print("-----------------------------------")

    return burt_matrix


def build_clusters_attributes(data_directory, clusters, attributes, timesteps_graphs):
    """
    builds the attributes for the clusters by taking the mean of the attributes of the vertices of each clusters
    :param data_directory: the directory from which to load the data
    :param clusters: the list of clusters
    :param attributes: the attributes of the vertices
    :param timesteps_graphs: the list of timestep graphs
    :return:
    """

    # Generate the clusters attributs
    # by taking the average of the attributes of the nodes that form the cluster
    clusters_attributes = []
    timestep_index = 0
    for timestep in clusters:
        graph = timesteps_graphs[timestep_index]
        node_list = np.array(list(graph.nodes()))
        # degree_centrality = nx.eigenvector_centrality(graph)
        for cluster in timestep:
            cluster_attributes = []
            for node in cluster:
                # node_attributes = [i * degree_centrality[node] for i in attributes[timestep_index][np.where(node_list == node)[0][0]]]
                node_attributes = attributes[timestep_index][np.where(node_list == node)[0][0]]
                cluster_attributes.append(node_attributes)
            mean_attributes = np.mean(cluster_attributes, axis=0)
            median_attributes = np.median(cluster_attributes, axis=0)
            std_attributes = np.std(cluster_attributes, axis=0)
            skew_attributes = st.skew(cluster_attributes, axis=0)
            kurtosis_attributes = st.kurtosis(cluster_attributes, axis=0)

            clusters_attributes.append(mean_attributes)

        timestep_index += 1

    # save the numpy array
    np.save(data_directory + "\\processed\\clusters_attributes", clusters_attributes)
    return clusters_attributes


def load_preprocessed_data(param_data_directory, param_with_attributes, param_with_clusters,
                           param_with_ground_truth_sequences):
    """
    Load preporcessed data into memory
    :param param_data_directory: data source directory
    :param param_with_attributes: If the graph is attributed
    :param param_with_clusters: if the clusters are provided with the dataset
    :param param_with_ground_truth_sequences: if the ground truth sequences are provided with the graph
    :return:
    """
    global data_directory
    global with_attributes
    global with_clusters
    global with_ground_truth_sequences
    global timesteps_graphs
    global clusters
    global attributes
    global clusters_attributes
    global clusters_topo_features
    global clusters_lookup
    global membership_matrix
    global burt_matrix
    global ground_truth
    global burt_norm
    global number_timesteps
    global number_clusters

    data_directory = param_data_directory

    with_attributes = param_with_attributes
    with_clusters = param_with_clusters
    with_ground_truth_sequences = param_with_ground_truth_sequences

    timesteps_graphs =  nx.read_gpickle(data_directory + "\\processed\\" + "timesteps_graphs" + ".nx")

    clusters = load_preprocessed_file(data_directory, "clusters")
    clusters_lookup = load_preprocessed_file(data_directory, "clusters_lookup")
    membership_matrix = load_preprocessed_file(data_directory, "membership_matrix")
    burt_matrix = load_preprocessed_file(data_directory, "burt_matrix")

    burt_norm = burt_matrix / burt_matrix.sum(axis=1)[:, None]
    number_timesteps = len(clusters)
    number_clusters = len(clusters_lookup)

    if with_ground_truth_sequences:
        ground_truth = ut.load_numpy_file(data_directory + "\\processed\\ground_truth.npy")

    if with_attributes:
        attributes = load_preprocessed_file(data_directory, "attributes")
        clusters_attributes = load_preprocessed_file(data_directory, "clusters_attributes")

    clusters_topo_features = load_preprocessed_file(data_directory, "clusters_topo_features")


def load_embeddings():
    """
    Load the generated embeddings in the pretraining step into memory
    :return:
    """
    global all_clusters_embeddings
    all_clusters_embeddings = load_preprocessed_file(data_directory, "all_clusters_embeddings")


def get_cluster_timestep(cluster):
    """
    returns the timestep id of a cluster
    :param cluster_lookup: the lookup object that indexes the clusters
    :param cluster: the id of the cluster
    :return: the id of the timestep
    """
    global clusters_lookup
    return (clusters_lookup[:][cluster])[0]


def get_clusters_in_timestep(timestep):
    """
    returns the list of custers per timestep
    :param cluster_lookup: the lookup object that indexes the clusters
    :param timestep: the id of the timestep
    :return: list of cluster ids
    """
    global clusters_lookup
    return [index for index in range(len(clusters_lookup)) if clusters_lookup[index][0] == timestep]


def get_sequence_clusters_in_timestep(sequence, timestep):
    """
    returns the list of clusters that belong to a certain sequence in a timestep
    :param cluster_lookup: the lookup object that indexes the clusters
    :param sequence: the sequence id grenated during tracking
    :param timestep: the id of the timestep
    :return: list of cluster ids
    """
    global clusters_lookup
    return [index for index in range(len(clusters_lookup)) if
            clusters_lookup[index][0] == timestep and index in sequence]


def get_sequence_clusters_in_previous_timesteps(sequence, timestep):
    """
    returns the list of clusters that belong to a certain sequence in previous timesteps
    :param cluster_lookup: the lookup object that indexes the clusters
    :param sequence: the sequence id grenated during tracking
    :param timestep: the id of the timestep
    :return: list of cluster ids
    """
    global clusters_lookup
    return [index for index in range(len(clusters_lookup)) if
            clusters_lookup[index][0] < timestep and index in sequence]
